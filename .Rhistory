for(i in 1:2){
data <- remove_outlier(data)
}
# generating the dummy variables
data <- data %>%
mutate_at(vars(Research, LOR, SOP, UniRatings), as.factor)
# Train dataset
set.seed(2)
train = sample(dim(data)[1], dim(data)[1]*0.7, replace = FALSE)
install.packages("tree")
library(tree)
tree_model <- tree( Admit ~ . , data, subset = train, split = "gini")
summary(tree_model)
plot(tree_model)
text(tree_model,pretty = 0)
# cross-validation for pruning
model_tree_cv <- cv.tree(tree_model,  FUN = prune.tree)
best = min(model_tree_cv$size[model_tree_cv$dev == min(model_tree_cv$dev)])
prune <- prune.tree(tree_model, best = best)
summary(prune)
# plotting tree
par(mfrow = c(1,2))
# dev = cross-validation error rate
# size = number of terminal nodes of each tree -> complexity
plot(model_tree_cv$size, model_tree_cv$dev, ylab = "cross-validation total rate", xlab = "Complexity = foglie", main ="Cross-validation total leaves")
abline(v = 8, col = "blue", lty = "dashed")
text(8, 15, "Benchmark", adj = c(0.5, -0.5))
plot(prune, main="Main title",
xlab="X axis title",
ylab="Y axis title",
sub="Sub-title")
text(prune, pretty = 0, main = "Tree pruned",  type = c("uniform"))
# clearing environment
rm(list = ls())
graphics.off()
# Add Libraries in order to use them
library(MASS)
library(ISLR2)
library(data.table)
library("corrplot")
library("ggplot2")
library("GGally")
library(pastecs)
library(psych)
library(dplyr)
library(performance)
library(lmtest)
library(boot)
library(glmnet)
# dataset cleaning
data <- read.csv("C:\\Users\\fsilv\\Dropbox\\UNI\\stistics\\1.csv")
data <- data.frame(data[,2:9])
colnames(data)[1] = "GRE"
colnames(data)[2] = "TOEFL"
colnames(data)[3] = "UniRatings"
colnames(data)[8] = "Admit"
# removing outliers
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe) {
columns <- names(dataframe)
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
for(i in 1:2){
data <- remove_outlier(data)
}
data <- data %>% relocate(CGPA, .after=TOEFL)
set.seed(1)
train = sample(dim(data)[1], dim(data)[1]*0.7, replace = FALSE)
df = data[train,]
data <- data %>%
mutate_at(vars(Research, LOR, SOP, UniRatings), as.factor)
library(boot)
set.seed(1)
df <- df %>%
mutate_at(vars(Research, LOR, SOP, UniRatings), as.factor)
get_model <- function(dataset, index){
boot_data <- dataset[index,]
boot_model <- glm(Admit ~ GRE + TOEFL + CGPA + LOR + Research, data = boot_data)
return (boot_model$coefficients)
}
boot_model <- boot(data = df, statistic = get_model, R = 1000)
boot_model <- boot(data = data[train,], statistic = get_model, R = 1000)
boot_model <- boot(data = data, statistic = get_model, R = 1000)
boot_model$t0
boot_model
boot_model <- boot(data = data[train,], statistic = get_model, R = 1000)
# clearing environment
rm(list = ls())
graphics.off()
# Add Libraries in order to use them
library(MASS)
library(ISLR2)
library(data.table)
library("corrplot")
library("ggplot2")
library("GGally")
library(pastecs)
library(psych)
library(dplyr)
library(performance)
library(lmtest)
library(boot)
library(glmnet)
# dataset cleaning
data <- read.csv("C:\\Users\\fsilv\\Dropbox\\UNI\\stistics\\1.csv")
data <- data.frame(data[,2:9])
colnames(data)[1] = "GRE"
colnames(data)[2] = "TOEFL"
colnames(data)[3] = "UniRatings"
colnames(data)[8] = "Admit"
# removing outliers
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe) {
columns <- names(dataframe)
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
for(i in 1:2){
data <- remove_outlier(data)
}
data <- data %>% relocate(CGPA, .after=TOEFL)
set.seed(1)
train = sample(dim(data)[1], dim(data)[1]*0.7, replace = FALSE)
df = data[train,]
data <- data %>%
mutate_at(vars(Research, LOR, SOP, UniRatings), as.factor)
lm_model <- lm(Admit ~ ., data = data, subset = train)
set.seed(1)
lm_fit_all <- lm(Admit ~ . + exp(GRE) + exp(TOEFL) + exp(CGPA) + log10(GRE) + log10(TOEFL) + log10(CGPA) + poly(GRE,3) + poly(TOEFL,3) + poly(CGPA,3) + sqrt(GRE) + sqrt(TOEFL) + sqrt(CGPA), data = data, subset = train)
summary(lm_fit_all)
# Step-wise Regression -> best model based on AIC value.
lm_fit_none <- lm(formula = Admit ~ 1, data = data, subset = train)
stepwise_model <- step(
object = lm_fit_all,
direction = "backward",
trace = FALSE)
summary(stepwise_model)
# model performance
performance <- compare_performance(lm_fit_train, lm_fit_all, stepwise_model)
as.data.frame(performance)
summary(stepwise_model)
# clearing environment
rm(list = ls())
graphics.off()
# Add Libraries in order to use them
library(MASS)
library(ISLR2)
library(data.table)
library("corrplot")
library("ggplot2")
library("GGally")
library(pastecs)
library(psych)
library(dplyr)
library(performance)
library(lmtest)
library(boot)
library(glmnet)
# dataset cleaning
data <- read.csv("C:\\Users\\fsilv\\Dropbox\\UNI\\stistics\\1.csv")
data <- data.frame(data[,2:9])
colnames(data)[1] = "GRE"
colnames(data)[2] = "TOEFL"
colnames(data)[3] = "UniRatings"
colnames(data)[8] = "Admit"
# removing outliers
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe) {
columns <- names(dataframe)
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
for(i in 1:2){
data <- remove_outlier(data)
}
data <- data %>% relocate(CGPA, .after=TOEFL)
set.seed(1)
train = sample(dim(data)[1], dim(data)[1]*0.7, replace = FALSE)
df = data[train,]
data <- data %>%
mutate_at(vars(Research, LOR, SOP, UniRatings), as.factor)
lm_model <- lm(Admit ~ ., data = data, subset = train)
summary(lm_model,digits=3)
lm_model <- update(lm_model, ~ . - SOP - UniRatings)
summary(lm_model,digits=3)
# analisys res -> train set
par(mfrow = c(1,4))
hist(lm_model$residuals,40,
xlab = "Residual",
main = "histogram residuals")
plot(lm_model$residuals, pch = "o", col = "blue" ,
ylab = "Residual", main = paste0("Residual plot - mean:",round(mean(lm_model$residuals),digits = 4),
"- var:", round(var(lm_model$residuals),digits = 4)))
abline(c(0,0),c(0,length(lm_model$residuals)), col= "red", lwd = 2)
boxplot(lm_model$residuals, ylab = "Residuals", main = "Outliers")$out
qqnorm(lm_model$residuals, main='Residuals')
qqline(lm_model$residuals)
shapiro.test(lm_model$residuals) # non normality
bptest(lm_model) # non homoscehdasticity
# Validation
yhat <- predict(lm_model, newdata = data[-train,])
lm_RMSE <- round(sqrt(mean((yhat - data$Admit[-train])^2)), digits = 4)
for(i in 1:dim(data.frame(yhat))[1]){
if(yhat[i] < 0 || yhat[i] > 1){
print(yhat[i])
}
}
df <- df %>%
mutate_at(vars(Research, LOR, SOP, UniRatings), as.factor)
get_model <- function(dataset, index){
boot_data <- dataset[index,]
boot_model <- glm(Admit ~ GRE + TOEFL + CGPA + LOR + Research, data = boot_data)
return (boot_model$coefficients)
}
boot_model <- boot(data = data, statistic = get_model, R = 1000)
boot_model$t0
boot_model
set.seed(1)
library(boot)
library(glmnet)
df <- data
for(x in 1:1:(dim(df)[2] - 5)){
df[paste("l",colnames(df)[x], sep = "")] <- log(df[x])
df[paste("s",colnames(df)[x], sep = "")] <- (df[x])^2
df[paste("c",colnames(df)[x], sep = "")] <- (df[x])^3
df[paste("r",colnames(df)[x], sep = "")] <- sqrt(df[x])
df[paste("e",colnames(df)[x], sep = "")] <- exp(df[x])
}
df[paste("e",colnames(df)[x], sep = "")] <- exp(df[x])
x <- model.matrix(Admit ~ . ,df) [, -1]
y <- df$Admit
lambda <- 10^seq(-3, 0,length = 300)
set.seed(1)
cv_lasso <- cv.glmnet(x[train, ], y[train], alpha = 1, lambda = lambda );
par(mfrow = c(1,1))
plot(cv_lasso)
coef(cv_lasso, cv_lasso$lambda.1se)
coef(cv_lasso, cv_lasso$lambda.min)
opt_lambda <- cv_lasso$lambda.1se
# clearing environment
rm(list = ls())
graphics.off()
# Add Libraries in order to use them
library(MASS)
library(ISLR2)
library(data.table)
library("corrplot")
library("ggplot2")
library("GGally")
library(pastecs)
library(psych)
library(dplyr)
library(performance)
library(lmtest)
library(boot)
library(glmnet)
# dataset cleaning
data <- read.csv("C:\\Users\\fsilv\\Dropbox\\UNI\\stistics\\1.csv")
data <- data.frame(data[,2:9])
colnames(data)[1] = "GRE"
colnames(data)[2] = "TOEFL"
colnames(data)[3] = "UniRatings"
colnames(data)[8] = "Admit"
# removing outliers
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe) {
columns <- names(dataframe)
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
for(i in 1:2){
data <- remove_outlier(data)
}
data <- data %>% relocate(CGPA, .after=TOEFL)
set.seed(1)
set.seed(1)
lm_fit_all <- lm(Admit ~ . + exp(GRE) + exp(TOEFL) + exp(CGPA) + log10(GRE) + log10(TOEFL) + log10(CGPA) + poly(GRE,3) + poly(TOEFL,3) + poly(CGPA,3) + sqrt(GRE) + sqrt(TOEFL) + sqrt(CGPA), data = data, subset = train)
summary(lm_fit_all)
# Step-wise Regression -> best model based on AIC value.
lm_fit_none <- lm(formula = Admit ~ 1, data = data, subset = train)
stepwise_model <- step(
object = lm_fit_all,
direction = "backward",
trace = FALSE)
set.seed(1)
lm_fit_all <- lm(Admit ~ . + exp(GRE) + exp(TOEFL) + exp(CGPA) + log10(GRE) + log10(TOEFL) + log10(CGPA) + poly(GRE,3) + poly(TOEFL,3) + poly(CGPA,3) + sqrt(GRE) + sqrt(TOEFL) + sqrt(CGPA), data = data, subset = train)
data <- data %>% relocate(CGPA, .after=TOEFL)
# clearing environment
rm(list = ls())
graphics.off()
# Add Libraries in order to use them
library(MASS)
library(ISLR2)
library(data.table)
library("corrplot")
library("ggplot2")
library("GGally")
library(pastecs)
library(psych)
library(dplyr)
library(performance)
library(lmtest)
library(boot)
library(glmnet)
# dataset cleaning
data <- read.csv("C:\\Users\\fsilv\\Dropbox\\UNI\\stistics\\1.csv")
data <- data.frame(data[,2:9])
colnames(data)[1] = "GRE"
colnames(data)[2] = "TOEFL"
colnames(data)[3] = "UniRatings"
colnames(data)[8] = "Admit"
# removing outliers
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe) {
columns <- names(dataframe)
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
for(i in 1:2){
data <- remove_outlier(data)
}
data <- data %>% relocate(CGPA, .after=TOEFL)
set.seed(1)
train = sample(dim(data)[1], dim(data)[1]*0.7, replace = FALSE)
df = data[train,]
data <- data %>%
mutate_at(vars(Research, LOR, SOP, UniRatings), as.factor)
# clearing environment
rm(list = ls())
graphics.off()
# Add Libraries in order to use them
library(MASS)
library(ISLR2)
library(data.table)
library("corrplot")
library("ggplot2")
library("GGally")
library(pastecs)
library(psych)
library(dplyr)
library(performance)
library(lmtest)
library(boot)
library(glmnet)
# dataset cleaning
data <- read.csv("C:\\Users\\fsilv\\Dropbox\\UNI\\stistics\\1.csv")
data <- data.frame(data[,2:9])
colnames(data)[1] = "GRE"
colnames(data)[2] = "TOEFL"
colnames(data)[3] = "UniRatings"
colnames(data)[8] = "Admit"
# removing outliers
detect_outlier <- function(x) {
Quantile1 <- quantile(x, probs=.25)
Quantile3 <- quantile(x, probs=.75)
IQR = Quantile3 - Quantile1
x > Quantile3 + (IQR*1.5) | x < Quantile1 - (IQR*1.5)
}
remove_outlier <- function(dataframe) {
columns <- names(dataframe)
for (col in columns) {
dataframe <- dataframe[!detect_outlier(dataframe[[col]]), ]
}
print("Remove outliers")
print(dataframe)
}
for(i in 1:2){
data <- remove_outlier(data)
}
data <- data %>% relocate(CGPA, .after=TOEFL)
set.seed(1)
train = sample(dim(data)[1], dim(data)[1]*0.7, replace = FALSE)
df = data[train,]
data <- data %>%
mutate_at(vars(Research, LOR, SOP, UniRatings), as.factor)
set.seed(1)
lm_fit_all <- lm(Admit ~ . + exp(GRE) + exp(TOEFL) + exp(CGPA) + log10(GRE) + log10(TOEFL) + log10(CGPA) + poly(GRE,3) + poly(TOEFL,3) + poly(CGPA,3) + sqrt(GRE) + sqrt(TOEFL) + sqrt(CGPA), data = data, subset = train)
summary(lm_fit_all)
lm_fit_all
lm_fit_all$coefficients
lm_fit_all$terms
lm_fit_all
lm_fit_all
summary(lm_fit_all)
View(lm_fit_all)
# Step-wise Regression -> best model based on AIC value.
lm_fit_none <- lm(formula = Admit ~ 1, data = data, subset = train)
stepwise_model <- step(
object = lm_fit_all,
direction = "backward",
trace = FALSE)
summary(stepwise_model)
View(stepwise_model)
model_cv_lasso = glm( asin(sqrt(Admit)) ~ . - SOP - UniRatings, data = data)
model_cv_lasso = cv.glm(data, lasso_model)
set.seed(1)
library(boot)
library(glmnet)
df <- data
for(x in 1:1:(dim(df)[2] - 5)){
df[paste("l",colnames(df)[x], sep = "")] <- log(df[x])
df[paste("s",colnames(df)[x], sep = "")] <- (df[x])^2
df[paste("c",colnames(df)[x], sep = "")] <- (df[x])^3
df[paste("r",colnames(df)[x], sep = "")] <- sqrt(df[x])
df[paste("e",colnames(df)[x], sep = "")] <- exp(df[x])
}
df[paste("e",colnames(df)[x], sep = "")] <- exp(df[x])
x <- model.matrix(Admit ~ . ,df) [, -1]
y <- df$Admit
lambda <- 10^seq(-3, 0,length = 300)
set.seed(1)
cv_lasso <- cv.glmnet(x[train, ], y[train], alpha = 1, lambda = lambda );
par(mfrow = c(1,1))
plot(cv_lasso)
coef(cv_lasso, cv_lasso$lambda.1se)
coef(cv_lasso, cv_lasso$lambda.min)
opt_lambda <- cv_lasso$lambda.1se
lasso_model <- glmnet(x[train,], y[train], alpha = 1, lambda = opt_lambda)
fitt_value <- predict(lasso_model, s = opt_lambda, newx=x[-train,])
lasso_RMSE_test = round(sqrt(mean((y[-train] - fitt_value)^2)), digits = 4)
res_lasso = y[-train] - fitt_value
coef(lasso_model, opt_lambda)
# Testing residuals
shapiro.test(res_lasso) # non normality
#bptest() # vale solo nel caso lineare
lasso_Rsquadre = lasso_model$dev.ratio
par(mfrow = c(1,4))
hist(res_lasso,40,
xlab = "Residual",
main = "Distribuzione empirica dei residui")
plot(res_lasso, pch = "o", col = "blue" ,
ylab = "Residual", main = paste0("Residual plot - mean:",round(mean(res_lasso),digits = 4),
"- var:", round(var(res_lasso),digits = 4)))
abline(c(0,0),c(0,length(res_lasso)), col= "red", lwd = 2)
boxplot(res_lasso, ylab = "Residuals", main = "Outliers", )$out
qqnorm(res_lasso, main='Residuals')
qqline(res_lasso)
model_cv_lasso = glm( asin(sqrt(Admit)) ~ . - SOP - UniRatings, data = data)
model_cv_lasso = cv.glm(data, lasso_model)
model_cv_lasso = cv.glm(data, lasso_model)
model_cv_lasso = glm( x[train,], y[train], alpha = 1, lambda = opt_lambda)
model_cv_lasso = glmnet(x[train,], y[train], alpha = 1, lambda = opt_lambda)
model_cv_lasso = cv.glm(data, model_cv_lasso)
model_cv_lasso = cv.glm(data, lasso_model)
lasso_model
res_lasso = y[-train] - fitt_value
coef(lasso_model, opt_lambda)
model_cv_lasso = glmnet(Admit ~ sGRE + cGRE + TOEFL + sTOEFL + cTOEFL + CGPA + Research)
model_cv_lasso = glmnet(Admit ~ sGRE + cGRE + TOEFL + sTOEFL + cTOEFL + CGPA + Research, data = df)
model_cv_lasso = glm(Admit ~ sGRE + cGRE + TOEFL + sTOEFL + cTOEFL + CGPA + Research, data = df)
model_cv_lasso = cv.glm(data, lasso_model)
model_cv_lasso = glm(Admit ~ sGRE + cGRE + TOEFL + sTOEFL + cTOEFL + CGPA + Research, data = df)
model_cv_lasso = cv.glm(data, model_cv_lasso)
df$sGRE
model_cv_lasso = glm(df$Admit ~ df$sGRE + df$cGRE + df$TOEFL + df$sTOEFL + df$cTOEFL + df$CGPA + df$Research, data = df)
model_cv_lasso = cv.glm(data, model_cv_lasso)
model_imp_rMSE_lasso <- round(sqrt(model_cv_lasso$delta[1]), digits = 4)
model_imp_rMSE_lasso <- round(sqrt(model_cv_lasso$delta[2]), digits = 4)
model_imp_rMSE_lasso_train <- round(sqrt(model_cv_lasso$delta[1]), digits = 4)
model_imp_rMSE_lasso_test <- round(sqrt(model_cv_lasso$delta[2]), digits = 4)
coef(lasso_model, opt_lambda)
View(df)
model_cv_lasso = glm(df$Admit ~ df$sGRE + df$cGRE + df$TOEFL + df$sTOEFL + df$cTOEFL + df$CGPA + df$Research, data = df)
model_cv_lasso = cv.glm(df, model_cv_lasso)
model_imp_rMSE_lasso_train <- round(sqrt(model_cv_lasso$delta[1]), digits = 4)
model_imp_rMSE_lasso_test <- round(sqrt(model_cv_lasso$delta[2]), digits = 4)
model_cv_lasso = glm(Admit ~ (GRE)^2 + (GRE)^3 + TOEFL + (TOEFL)^2 + (TOEFL)^3 + CGPA + Research, data = data)
model_cv_lasso = cv.glm(data, model_cv_lasso)
model_imp_rMSE_lasso_train <- round(sqrt(model_cv_lasso$delta[1]), digits = 4)
model_imp_rMSE_lasso_test <- round(sqrt(model_cv_lasso$delta[2]), digits = 4)
View(df)
